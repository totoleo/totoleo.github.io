---
layout: post
title: "CS224d: Deep Learning for Natural Language Processing"
description: "自然语言深度学习的课程笔记"
category: nlp
tags: [技术,机器学习,NLP,Deep Learning]
---
先放上这门课程的[课件地址](http://cs224d.stanford.edu/syllabus.html)和[视频](https://www.youtube.com/watch?v=xhHOL3TNyJs&list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam)（需科学上网）  

今天看完了Lecture 1，记录一下收获和疑问。  

这篇Lecture作为开课的第一节，主要介绍为什么需要使用Deep Learning做自然语言处理(NLP)，以及在NLP中如何应用Deep Learning。   

其实Deep Learning这个东西不是一个特别新鲜的概念和方法。之前在吴军教授的《数学之美》中就了解到Google翻译是如何做文本翻译的，大概了解到使用概率模型解决翻译问题的方法。细节比较模糊，正好在这里补充一下。  

矩阵向量，在XXX介绍完Deep Learning之后，这个词就非常频繁的出现。具体这个向量是怎么构造的没GET到(自己的听力还是够差劲的)。笔记中有相应的介绍。

仔细看了一下笔记，才知道这个矩阵叫[共生矩阵(Co-occurrence Matrix)](https://en.wikipedia.org/wiki/Co-occurrence_matrix)（具体什么东东，估计又要百科一下了）。

刚看了一下这个百科，原来共生矩阵代图像中所有值中任意两个值在某个方向(Δx,Δy)共同出现的次数。这个矩阵表现了图像在这个方向上的相似性。而对于DL，它定义的「图像」是采集的文本，方向通常是(Δx,0)。
